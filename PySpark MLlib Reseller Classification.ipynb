{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "application/vnd.databricks.v1+notebook": {
            "dashboards": [],
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "PySpark Reseller Classification v2",
            "widgets": {}
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Machine Learning - Reseller Classification\n",
                "\n",
                "## Project Scope:\n",
                "\n",
                "The purpose of this project is to distinguish the resellers from the customers who purchased on the US website. Resellers harm the brand reputation, product market price, and inventory management of the company. Therefore, the company wants to identify and block the resellers' shipments.\n",
                "\n",
                "The project involves a binary classification problem that may require feature engineering if necessary. The data source is a structured dataset in the database, which includes numerical and categorical features.\n",
                "\n",
                "## Dataset column descriptions:\n",
                "\n",
                "      'sales\\_channel\\_id': US sales channel id is 1, integer \n",
                "\n",
                "      'external\\_customer\\_id': customer id, integer \n",
                "\n",
                "      'email': customer email address, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_address1': the shipping address 1 used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_address2': the shipping address 2 used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_city': the shipping city used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_zip': the shipping address zip code used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_country\\_code': the shipping country code used in the last transaction, string\n",
                "\n",
                "      'total\\_orders': the total count of orders purchased by the customer, integer \n",
                "\n",
                "      'total\\_units': the total count of item units purchased by the customer, integer \n",
                "\n",
                "      'total\\_gross': the total gross sales spent by the customer, float\n",
                "\n",
                "      'total\\_discounts': the total discounts used by the customer, float\n",
                "\n",
                "      'total\\_returns': the total returns to the customer, float\n",
                "\n",
                "      'total\\_shipping': the total shipping spent by the customer, float\n",
                "\n",
                "      'total\\_taxes': the total taxes purchased by the customer, float\n",
                "\n",
                "      'r\\_score': recency score represents how recently a customer has made a purchase, score 1-5, integer \n",
                "\n",
                "      'f\\_score': frequency score represents how often a customer makes a purchase, score 1-5, integer \n",
                "\n",
                "      'm\\_score': monetary value score represents how much money a customer spends on purchases, score 1-5, integer\n",
                "\n",
                "      'rfm\\_score': r\\_score + f\\_score + m\\_score, integer \n",
                "\n",
                "      'is\\_reseller': 1 (reseller) or 0 (normal customer), this is the target, integer\n",
                "\n",
                "## Preliminary Analysis:\n",
                "\n",
                "The main features that are considered for the analysis are:\n",
                "\n",
                "\\- Total orders, units, gross sales, and discounts: These features reflect the reseller behavior of buying large quantities of products during the discount season.\n",
                "\n",
                "\\- Total returns: This feature indicates the reseller tendency of returning unsold products.\n",
                "\n",
                "\\- Total shipping and taxes: These features provide some information about the reseller location.\n",
                "\n",
                "\\- R\\_score, F\\_score, M\\_score, RFM\\_score: These features are derived from the recency, frequency, and monetary value of each customer's purchases and may help in training a model. The training process utilizes only R\\_score, F\\_score, and RFM\\_score as the input features. This is based on the rationale that these three features encompass the information of M\\_score. Including M\\_score as an additional feature would result in a correlation problem.\n",
                "\n",
                "  \n",
                "\n",
                "One challenge in the reseller classification problem is to distinguish between loyal customers and resellers. Loyal customers spend a lot of money in total, but each transaction does not include many units. Resellers purchase multiple units in a single transaction. Therefore, two custom features are created to capture this difference:\n",
                "\n",
                "\\- Average units per order: This feature is obtained by dividing total units by total orders.\n",
                "\n",
                "\\- Average gross sales per order: This feature is obtained by dividing total gross sales by total orders.\n",
                "\n",
                "The resellers may follow a different pattern in these two custom features compared to the loyal customers.\n",
                "\n",
                "The preliminary training stage involves training a model with all the numerical features plus the two custom features. The model will be trained using PySpark MLlib and Keras Deep Learning, and the performance of different classifiers will be evaluated. This notebook focuses on PySpark MLlib models. The Keras Deep Learning models will be demonstrated in other notebooks going forward.\n",
                "\n",
                "## Improvements:\n",
                "\n",
                "Based on the feedback from the team who identified the resellers, I have analyzed the following criteria: email address, shipping address, and IP address. I have discovered that some resellers use multiple email accounts and vary their shipping addresses to avoid detection. However, these methods can be exposed by examining the email domain name and the embedding shipping address of the orders. The IP address is not a reliable indicator, as it can be easily changed by using a VPN. Therefore, I propose to create a model that considers both numerical and categorical features (excludes IP address) of the orders, and uses an embedding space to measure the distance between different shipping addresses. This will help us to detect the resellers' intentions more accurately."
            ],
            "metadata": {
                "azdata_cell_guid": "837e3d4c-8a1b-48fa-bf2f-444e9d58c220"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Define input and output"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "5058c660-6a3a-40b1-973d-95780d2f5c0d",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "a11864f2-fd05-4d07-a3c8-79f5e9a8dfd4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "model_name = \"Reseller Classifier\"\n",
                "input_table_name = \"customer\"\n",
                "output_table_name = \"ml_resellers\""
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0171b7de-9607-48c3-813d-b122862960c0",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "e07ce754-ea13-437f-bf01-cefef8ddac11",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# load table as a Spark DataFrame\n",
                "customers = (spark.read\n",
                "  .format(\"jdbc\")\n",
                "  .option(\"url\", jdbcUrl)\n",
                "  .option(\"dbtable\", input_table_name)\n",
                "  .option(\"user\", user)\n",
                "  .option(\"password\", password)\n",
                "  .load()\n",
                ")\n",
                "\n",
                "# take only US 2023 data\n",
                "customers = customers.where('sales_channel_id = 1 AND CAST(last_transaction_date AS DATE) between \"2023-01-01\" and \"2023-05-23\" ')"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "29559893-4dca-4492-9760-be5292a2f102",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "0ef6f613-5a65-4f12-8a00-7308e5402822",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "input_columns = ['sales_channel_id'\n",
                "      ,'external_customer_id'\n",
                "      ,'email'\n",
                "      ,'last_shipping_address_address1'\n",
                "      ,'last_shipping_address_address2'\n",
                "      ,'last_shipping_address_city'\n",
                "      ,'last_shipping_address_zip'\n",
                "      ,'last_shipping_address_country_code'\n",
                "      ,'total_orders'\n",
                "      ,'total_units'\n",
                "      ,'total_gross'\n",
                "      ,'total_discounts'\n",
                "      ,'total_returns'\n",
                "      ,'total_shipping'\n",
                "      ,'total_taxes'\n",
                "      ,'r_score'\n",
                "      ,'f_score'\n",
                "      ,'rfm_score'\n",
                "      ,'is_reseller'\n",
                "      ]"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ea4fd7c0-4493-4366-849a-e0148436294f",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "5b885073-7d46-4de2-b6f7-865e0fcb883f",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create Custom Features"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "deae9041-4f64-4b39-9c37-de9fcba01b68",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "7e354eeb-c3d5-49f7-ba62-7ec72113d3eb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pyspark.sql.functions as F\n",
                "\n",
                "df = customers.select(\n",
                "    [customers[col] for col in input_columns]\n",
                "    + [\n",
                "        F.substring_index(\n",
                "            F.substring_index(F.lower(customers[\"email\"]), \"@\", -1), \".\", 1\n",
                "        ).alias(\"email_domain\")\n",
                "    ]\n",
                "    + [\n",
                "        F.concat(\n",
                "            F.coalesce(F.lower(customers[\"last_shipping_address_address1\"]), F.lit(\"\")),\n",
                "            F.lit(\" \"),\n",
                "            F.coalesce(F.lower(customers[\"last_shipping_address_address2\"]), F.lit(\"\")),\n",
                "            F.lit(\" \"),\n",
                "            F.coalesce(F.lower(customers[\"last_shipping_address_city\"]), F.lit(\"\")),\n",
                "            F.lit(\" \"),\n",
                "            F.coalesce(F.lower(customers[\"last_shipping_address_country_code\"]), F.lit(\"\")),\n",
                "            F.lit(\" \"),\n",
                "            F.coalesce(F.lower(customers[\"last_shipping_address_zip\"]), F.lit(\"\")),\n",
                "        ).alias(\"address\")\n",
                "    ]\n",
                ").withColumn(\"email_domain_address\", F.array(\"email_domain\", \"address\"))"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "e0668a98-b3a8-451b-ab6f-b607db182ee5",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "f33b771e-c94f-4715-8cdb-aef060fe1c90",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Data Cleansing"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "1ae8c49d-b1a1-4275-8e6f-3f5461d97bd3"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "IDENTIFIERS = ['external_customer_id', 'email']\n",
                "CONTINUOUS_COLUMNS = [\n",
                "  'total_orders',\n",
                "  'total_units',\n",
                "  'total_gross',\n",
                "  'total_discounts',\n",
                "  'total_returns',\n",
                "  'total_shipping',\n",
                "  'total_taxes',\n",
                "  'r_score',\n",
                "  'f_score',\n",
                "  'rfm_score'\n",
                "]\n",
                "CATEGORICAL_COLUMN = 'email_domain_address'\n",
                "TARGET_COLUMN = ['is_reseller']"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b05c1d72-4dfb-4492-b918-c2c9126d54c3",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "d902f0ce-9693-4c82-849a-a0dc6be6ae31",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "customers = df.dropna(\n",
                "  how='any',\n",
                "  subset=[x for x in IDENTIFIERS + CONTINUOUS_COLUMNS + TARGET_COLUMN] + [CATEGORICAL_COLUMN]\n",
                ")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "adeb6073-2936-4cc0-8778-ecd18aa89310",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "0c5ecf70-ccf9-413e-b2f9-15fa4afe8cfe",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "customers = customers.withColumn(\n",
                "  'units_per_order', F.col('total_units') * 1.0 / F.col('total_orders')\n",
                ").withColumn(\n",
                "  'gross_per_order', F.col('total_gross') * 1.0 / F.col('total_orders')\n",
                ")\n",
                "\n",
                "customers = customers.fillna(0.0, subset=['units_per_order', 'gross_per_order'])\n",
                "\n",
                "CONTINUOUS_COLUMNS += ['units_per_order', 'gross_per_order']"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "2a788f1a-bd6b-41f3-950b-6c036c8f1e10",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "2859bbbf-0b4c-4c5a-84b9-2fbe79a2ce8c",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "CONTINUOUS_COLUMNS"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "bb606022-e26f-4b27-acaf-2d8fcef9ed4a",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "96cce205-c842-4390-964d-df69b900661e",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 29,
                    "data": {
                        "text/plain": "['total_orders',\n 'total_units',\n 'total_gross',\n 'total_discounts',\n 'total_returns',\n 'total_shipping',\n 'total_taxes',\n 'r_score',\n 'f_score',\n 'rfm_score',\n 'units_per_order',\n 'gross_per_order']"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Train Test Split"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "76fd7d86-f0fa-43ec-ac45-6db2c40ee8d7",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "c14f5815-a5f1-4682-b658-d89539d5c5ee"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "resellers = customers.where('sales_channel_id = 1 and is_reseller = 1')\n",
                "normal_customers = customers.where('sales_channel_id = 1 and is_reseller = 0')\n",
                "\n",
                "train_resellers, test_resellers, val_resellers = resellers.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
                "train_normal, test_normal, val_normal = normal_customers.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
                "\n",
                "train = train_resellers.union(train_normal).orderBy(F.rand(seed=42))\n",
                "test = test_resellers.union(test_normal).orderBy(F.rand(seed=42))\n",
                "val = val_resellers.union(val_normal).orderBy(F.rand(seed=42))\n",
                "\n",
                "train.cache()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "376c2cd5-8865-418e-a096-415b9ce6ca48",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "7e0c7263-461f-4216-80ff-6b731d1acbe0",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 30,
                    "data": {
                        "text/plain": "DataFrame[sales_channel_id: int, external_customer_id: bigint, customer_type: string, email: string, first_name: string, last_name: string, last_shipping_address_address1: string, last_shipping_address_address2: string, last_shipping_address_city: string, last_shipping_address_country: string, last_shipping_address_phone: string, last_shipping_address_province: string, last_shipping_address_zip: string, last_shipping_address_country_code: string, last_shipping_address_province_code: string, first_transaction_date: timestamp, first_transaction_id: bigint, last_transaction_date: timestamp, last_transaction_id: bigint, last_transaction_ip: string, total_orders: int, total_units: int, total_gross: decimal(19,4), total_discounts: decimal(19,4), total_returns: decimal(19,4), total_shipping: decimal(19,4), total_taxes: decimal(19,4), r_score: int, f_score: int, m_score: int, rfm_score: int, is_reseller: int, email_domain: string, address: string, email_domain_address: array<string>, units_per_order: double, gross_per_order: double]"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create a Pipeline"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "a95dcbe7-2a87-4361-8fc7-fc438cb8a832",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "dc4ef9de-e91e-4f10-a056-817e08707b1b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "imput_columns = [(x + '_i') for x in CONTINUOUS_COLUMNS if x not in ['units_per_order', 'gross_per_order']]"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0edca4f9-fc28-40d1-b5ba-bf34a1d9fbd3",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "aa0768e0-0b0a-40ab-8124-fc965f9513dc",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml import Pipeline\n",
                "import pyspark.ml.feature as MF\n",
                "\n",
                "embedding_size = 100\n",
                "\n",
                "imputer = MF.Imputer(\n",
                "  strategy='mean',\n",
                "  inputCols=[x for x in CONTINUOUS_COLUMNS if x not in ['units_per_order', 'gross_per_order']],\n",
                "  outputCols=imput_columns\n",
                ")\n",
                "\n",
                "embedding = MF.Word2Vec(\n",
                "    vectorSize=embedding_size,\n",
                "    inputCol='email_domain_address',\n",
                "    outputCol='embedded'\n",
                ")\n",
                "\n",
                "continuous_assembler = MF.VectorAssembler(\n",
                "  inputCols=imput_columns + ['units_per_order', 'gross_per_order', 'embedded'],\n",
                "  outputCol='continuous'\n",
                ")\n",
                "\n",
                "continuous_scaler = MF.StandardScaler(\n",
                "  inputCol='continuous',\n",
                "  outputCol='features'\n",
                ")\n",
                "\n",
                "customers_pipeline = Pipeline(\n",
                "  stages=[imputer, embedding, continuous_assembler, continuous_scaler]\n",
                ")\n",
                "\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "customers_features = customers_pipeline_model.transform(train)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "5d3cf9eb-d9da-438e-8353-baf6f4ef3a2b",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "24b3baeb-3704-4a5f-a11b-ccb883602b33",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##LogisticRegression"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "88cc1de2-40b7-44cf-b39a-3240dbf6e53b",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "c8d2bf68-533e-43a3-9162-74a97b73532f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.classification import LogisticRegression\n",
                "import mlflow\n",
                "\n",
                "clf = LogisticRegression(\n",
                "  featuresCol='features',\n",
                "  labelCol='is_reseller',\n",
                "  predictionCol='prediction'\n",
                ")\n",
                "\n",
                "customers_pipeline.setStages(\n",
                "  [\n",
                "    imputer, \n",
                "    embedding,\n",
                "    continuous_assembler, \n",
                "    continuous_scaler,\n",
                "    clf\n",
                "  ]\n",
                ")\n",
                "\n",
                "# Start an MLflow run\n",
                "mlflow.start_run()\n",
                "\n",
                "# Train a model\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "\n",
                "# Predictions\n",
                "results = customers_pipeline_model.transform(val)\n",
                "\n",
                "# Log metrics\n",
                "model = customers_pipeline_model.stages[-1]\n",
                "metrics = model.evaluate(results.select('email', 'is_reseller', 'features'))\n",
                "\n",
                "mlflow.log_metric(\"val_accuracy\", metrics.accuracy)\n",
                "mlflow.log_metric(\"val_precision\", metrics.precisionByLabel[1])\n",
                "mlflow.log_metric(\"val_recall\", metrics.recallByLabel[1])\n",
                "\n",
                "# Log the model\n",
                "mlflow.spark.log_model(customers_pipeline_model, \"LogisticRegression_model\")\n",
                "\n",
                "# Close the MLflow run\n",
                "mlflow.end_run()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "9fcec4be-c1d2-4b64-aa64-19346163ee43",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "ed687d9c-931e-45b5-b367-bc5041848222",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/05/25 02:56:25 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##RandomForest"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "2c377111-46c9-4a8b-bcfe-023761642465",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "897c41fe-fc32-4ca2-abb0-96c6278af5ac"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.classification import RandomForestClassifier\n",
                "\n",
                "clf = RandomForestClassifier(\n",
                "  featuresCol='features',\n",
                "  labelCol='is_reseller',\n",
                "  predictionCol='prediction'\n",
                ")\n",
                "\n",
                "customers_pipeline.setStages(\n",
                "  [\n",
                "    imputer, \n",
                "    embedding,\n",
                "    continuous_assembler, \n",
                "    continuous_scaler,\n",
                "    clf\n",
                "  ]\n",
                ")\n",
                "\n",
                "# Start an MLflow run\n",
                "mlflow.start_run()\n",
                "\n",
                "# Train a model\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "\n",
                "# Predictions\n",
                "results = customers_pipeline_model.transform(val)\n",
                "\n",
                "# Log metrics\n",
                "model = customers_pipeline_model.stages[-1]\n",
                "metrics = model.evaluate(results.select('email', 'is_reseller', 'features'))\n",
                "\n",
                "mlflow.log_metric(\"val_accuracy\", metrics.accuracy)\n",
                "mlflow.log_metric(\"val_precision\", metrics.precisionByLabel[1])\n",
                "mlflow.log_metric(\"val_recall\", metrics.recallByLabel[1])\n",
                "\n",
                "# Log the model\n",
                "mlflow.spark.log_model(customers_pipeline_model, \"RandomForest_model\")\n",
                "\n",
                "# Close the MLflow run\n",
                "mlflow.end_run()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "47a13ce5-7ce5-4633-877e-2bf67f759753",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "1dc2a1d0-bebe-48da-ab8b-e30823f9a861",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/05/25 04:57:59 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##GBTClassifier"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d64d9757-85cf-4e6f-ab1b-16f5b4fc55f1",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "a35cd9af-08e6-4da2-a67f-3135a296d007"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import mlflow\n",
                "\n",
                "from pyspark.ml.classification import GBTClassifier\n",
                "from pyspark.mllib.evaluation import MulticlassMetrics\n",
                "\n",
                "clf = GBTClassifier(\n",
                "  featuresCol='features',\n",
                "  labelCol='is_reseller',\n",
                "  predictionCol='prediction'\n",
                ")\n",
                "\n",
                "customers_pipeline.setStages(\n",
                "  [\n",
                "    imputer, \n",
                "    embedding,\n",
                "    continuous_assembler, \n",
                "    continuous_scaler,\n",
                "    clf\n",
                "  ]\n",
                ")\n",
                "\n",
                "# Start an MLflow run\n",
                "mlflow.start_run()\n",
                "\n",
                "# Train a model\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "\n",
                "# Predictions\n",
                "results = customers_pipeline_model.transform(val)\n",
                "\n",
                "# Extract the predicted labels and true labels from the predictions DataFrame\n",
                "predictionAndLabels = results.select('prediction', F.col('is_reseller').cast(\"double\")).rdd\n",
                "\n",
                "# Create MulticlassMetrics object\n",
                "metrics = MulticlassMetrics(predictionAndLabels)\n",
                "\n",
                "# Log metrics\n",
                "mlflow.log_metric(\"val_accuracy\", metrics.accuracy)\n",
                "mlflow.log_metric(\"val_precision\", metrics.precision(1))\n",
                "mlflow.log_metric(\"val_recall\", metrics.recall(1))\n",
                "\n",
                "# Log the model\n",
                "mlflow.spark.log_model(customers_pipeline_model, \"GBT_model\")\n",
                "\n",
                "# Close the MLflow run\n",
                "mlflow.end_run()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "dba82023-35b8-4ceb-a731-0659c90841b0",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "55a8655b-62cd-4013-8fd7-74f57265e28f",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n2023/05/25 19:13:03 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##Load Best Model"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "fa403485-f6cc-487f-ae83-d68628b65999",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "a9ed2b29-4d05-47e0-8a6e-ff74b0463f3e"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Validation dataset Evaluation"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "0ac16159-80f5-453e-8673-1b8d6e24afaa",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "ebc4bc8b-8288-49d6-b69e-86a19effed76"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import mlflow\n",
                "logged_model = 'runs:/GBT_model'\n",
                "\n",
                "# Load model\n",
                "loaded_model = mlflow.spark.load_model(logged_model)\n",
                "\n",
                "# Perform inference via model.transform()\n",
                "results = loaded_model.transform(val)\n",
                "\n",
                "# Extract the predicted labels and true labels from the predictions DataFrame\n",
                "predictionAndLabels = results.select('prediction', F.col('is_reseller').cast(\"double\")).rdd\n",
                "\n",
                "# Create MulticlassMetrics object\n",
                "metrics = MulticlassMetrics(predictionAndLabels)\n",
                "\n",
                "# validation metrics\n",
                "print(metrics.accuracy, metrics.precision(1), metrics.recall(1))"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "98935c30-e09b-43da-a78a-28a5c7a263a3",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "038c41d5-ac6f-43ed-8a0c-d8547c23d50b",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/05/25 03:16:53 INFO mlflow.spark: 'runs:/a2b45aae0d854e5b81cb34caebd12e07/GBT_model' resolved as 'dbfs:/databricks/mlflow-tracking/4401469133110519/a2b45aae0d854e5b81cb34caebd12e07/artifacts/GBT_model'\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.9992584987451517 0.9090909090909091 0.75\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Test dataset Evaluation"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ea6709ec-cbac-4380-bb31-faebbcf55bad",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "16c5d18b-869a-45fd-92cc-d5ade600f0d7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import mlflow\n",
                "logged_model = 'runs:/GBT_model'\n",
                "\n",
                "# Load model\n",
                "loaded_model = mlflow.spark.load_model(logged_model)\n",
                "\n",
                "# Perform inference via model.transform()\n",
                "results = loaded_model.transform(test)\n",
                "\n",
                "# Extract the predicted labels and true labels from the predictions DataFrame\n",
                "predictionAndLabels = results.select('prediction', F.col('is_reseller').cast(\"double\")).rdd\n",
                "\n",
                "# Create MulticlassMetrics object\n",
                "metrics = MulticlassMetrics(predictionAndLabels)\n",
                "\n",
                "# validation metrics\n",
                "print(metrics.accuracy, metrics.precision(1), metrics.recall(1))"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b19991fa-c7ad-47cb-8f1d-e78180b5ac5c",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "a04ec36c-aae7-42b9-a288-b70c9ed83f48",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/05/25 03:17:13 INFO mlflow.spark: 'runs:/a2b45aae0d854e5b81cb34caebd12e07/GBT_model' resolved as 'dbfs:/databricks/mlflow-tracking/4401469133110519/a2b45aae0d854e5b81cb34caebd12e07/artifacts/GBT_model'\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.9994229326562409 0.9333333333333333 0.7777777777777778\n"
                }
            ],
            "execution_count": null
        }
    ]
}
