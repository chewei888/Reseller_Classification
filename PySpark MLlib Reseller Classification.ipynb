{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "application/vnd.databricks.v1+notebook": {
            "dashboards": [],
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "PySpark Reseller Classification v2",
            "widgets": {}
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Machine Learning - Reseller Classification\n",
                "\n",
                "## Project Scope:\n",
                "\n",
                "The purpose of this project is to distinguish the resellers from the customers who purchased on the US website. Resellers harm the brand reputation, product market price, and inventory management of the company. Therefore, the company wants to identify and block the resellers' shipments.\n",
                "\n",
                "The project involves a binary classification problem that may require feature engineering if necessary. The data source is a structured dataset in the database, which includes numerical and categorical features.\n",
                "\n",
                "The training of the model will be carried out using PySpark MLlib and Keras Deep Learning frameworks, followed by an evaluation of various classifiers. This notebook is specifically dedicated to exploring three unique PySpark MLlib models: Logistic Regression, Random Forest, and Gradient Boosted Trees (GBT). The demonstration of Keras Deep Learning models will be covered in subsequent notebooks.\n",
                "\n",
                "## Dataset column descriptions:\n",
                "\n",
                "      'sales\\_channel\\_id': US sales channel id is 1, integer \n",
                "\n",
                "      'external\\_customer\\_id': customer id, integer \n",
                "\n",
                "      'email': customer email address, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_address1': the shipping address 1 used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_address2': the shipping address 2 used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_city': the shipping city used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_zip': the shipping address zip code used in the last transaction, string\n",
                "\n",
                "      'last\\_shipping\\_address\\_country\\_code': the shipping country code used in the last transaction, string\n",
                "\n",
                "      'total\\_orders': the total count of orders purchased by the customer, integer \n",
                "\n",
                "      'total\\_units': the total count of item units purchased by the customer, integer \n",
                "\n",
                "      'total\\_gross': the total gross sales spent by the customer, float\n",
                "\n",
                "      'total\\_discounts': the total discounts used by the customer, float\n",
                "\n",
                "      'total\\_returns': the total returns to the customer, float\n",
                "\n",
                "      'total\\_shipping': the total shipping spent by the customer, float\n",
                "\n",
                "      'total\\_taxes': the total taxes purchased by the customer, float\n",
                "\n",
                "      'r\\_score': recency score represents how recently a customer has made a purchase, score 1-5, integer \n",
                "\n",
                "      'f\\_score': frequency score represents how often a customer makes a purchase, score 1-5, integer \n",
                "\n",
                "      'm\\_score': monetary value score represents how much money a customer spends on purchases, score 1-5, integer\n",
                "\n",
                "      'rfm\\_score': r\\_score + f\\_score + m\\_score, integer \n",
                "\n",
                "      'is\\_reseller': 1 (reseller) or 0 (normal customer), this is the target, integer\n",
                "\n",
                "## Preliminary Analysis:\n",
                "\n",
                "The main features that are considered for the analysis are:\n",
                "\n",
                "\\- Total orders, units, gross sales, and discounts: These features reflect the reseller behavior of buying large quantities of products during the discount season.\n",
                "\n",
                "\\- Total returns: This feature indicates the reseller tendency of returning unsold products.\n",
                "\n",
                "\\- Total shipping and taxes: These features provide some information about the reseller location.\n",
                "\n",
                "\\- R\\_score, F\\_score, M\\_score, RFM\\_score: These features are derived from the recency, frequency, and monetary value of each customer's purchases and may help in training a model. The training process utilizes only R\\_score, F\\_score, and RFM\\_score as the input features. This is based on the rationale that these three features encompass the information of M\\_score. Including M\\_score as an additional feature would result in a correlation problem.\n",
                "\n",
                "One challenge in the reseller classification problem is to distinguish between loyal customers and resellers. Loyal customers spend a lot of money in total, but each transaction does not include many units. Resellers purchase multiple units in a single transaction. Therefore, two custom features are created to capture this difference:\n",
                "\n",
                "\\- Average units per order: This feature is obtained by dividing total units by total orders.\n",
                "\n",
                "\\- Average gross sales per order: This feature is obtained by dividing total gross sales by total orders.\n",
                "\n",
                "The resellers may follow a different pattern in these two custom features compared to the loyal customers.\n",
                "\n",
                "The preliminary training stage involves training a model with all the numerical features plus the two custom features. \n",
                "\n",
                "## Improvements 1:\n",
                "\n",
                "Based on the feedback from the team who identified the resellers, I have analyzed the following criteria: email address, shipping address, and IP address. I have discovered that some resellers use multiple email accounts and vary their shipping addresses to avoid detection. However, these methods can be exposed by examining the email domain name and the embedding shipping address of the orders. The IP address is not a reliable indicator, as it can be easily changed by using a VPN. Therefore, I propose to create a model that considers both numerical and categorical features (excludes IP address) of the orders, and uses an embedding space to measure the distance between different shipping addresses. This will help us to detect the resellers' intentions more accurately\n",
                "\n",
                "## Improvements 2:\n",
                "\n",
                "The observation reveals that resellers exhibit a tendency to procure a substantial number of units for each Stock Keeping Unit (SKU) and strive to acquire a wide variety of SKUs. This is primarily driven by their need for product diversity and the requirement to maintain ample stock levels in their warehouses. Resellers often divide their orders into smaller batches, resulting in a higher total unit count and distinct SKU count compared to regular customers. The latter group typically possesses personal preferences that dictate their selection of SKUs based on their individual style. In contrast, resellers lack such preferences as their objective is to cater to the diverse needs of various customers. Consequently, the implementation of two custom features can aid in identifying resellers. The first custom feature involves calculating the average distinct SKU count per order, which counts the number of unique SKUs across a customer's order history and dividing it by the total number of orders. The second custom feature, units per SKU, is computed by dividing the units per order by the average distinct SKU count per order. By incorporating these two custom features into the model training process, reseller classification can be significantly improved."
            ],
            "metadata": {
                "azdata_cell_guid": "22d678de-baf0-45e6-8388-a86f00243eed"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Define input and output"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "5058c660-6a3a-40b1-973d-95780d2f5c0d",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "bdfaf174-81da-4ce0-b420-2447a2cbbb9e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "model_name = \"Reseller Classifier\"\n",
                "input_table_name = \"customer\"\n",
                "output_table_name = \"ml_resellers\""
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0171b7de-9607-48c3-813d-b122862960c0",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "a9055f51-3468-452c-b631-04e7efff0038",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "import os\n",
                "\n",
                "# Define Azure SQL Database connection\n",
                "jdbcHostname = os.getenv(\"SQLDB_HOST\")\n",
                "user = os.getenv(\"SQLDB_USER\")\n",
                "password = os.getenv(\"SQLDB_PW\")\n",
                "jdbcDatabase = os.getenv(\"SQLDB_DB\")\n",
                "jdbcPort = 1433\n",
                "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
                "connectionProperties = {\n",
                "\"user\" : user,\n",
                "\"password\" : password,\n",
                "\"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
                "}"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0a6ec9a4-57eb-4296-bb42-386f8a285e36",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "6e4cf12d-e33d-4072-9d14-6624bed89bfa",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# load table as a Spark DataFrame\n",
                "customers = (spark.read\n",
                "  .format(\"jdbc\")\n",
                "  .option(\"url\", jdbcUrl)\n",
                "  .option(\"dbtable\", input_table_name)\n",
                "  .option(\"user\", user)\n",
                "  .option(\"password\", password)\n",
                "  .load()\n",
                ")\n",
                "\n",
                "# take only US 2023 data\n",
                "customers = customers.where('sales_channel_id = 1 AND CAST(last_transaction_date AS DATE) <= \"2023-05-28\" ')"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "29559893-4dca-4492-9760-be5292a2f102",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "40ae807b-91f2-4c61-842b-49c841634b85",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "input_columns = ['sales_channel_id'\n",
                "      ,'external_customer_id'\n",
                "      ,'customer_type'\n",
                "      ,'email'\n",
                "      ,'first_name'\n",
                "      ,'last_name'\n",
                "      ,'last_shipping_address_address1'\n",
                "      ,'last_shipping_address_address2'\n",
                "      ,'last_shipping_address_city'\n",
                "      ,'last_shipping_address_country'\n",
                "      ,'last_shipping_address_phone'\n",
                "      ,'last_shipping_address_province'\n",
                "      ,'last_shipping_address_zip'\n",
                "      ,'last_shipping_address_country_code'\n",
                "      ,'last_shipping_address_province_code'\n",
                "      ,'first_transaction_date'\n",
                "      ,'first_transaction_id'\n",
                "      ,'last_transaction_date'\n",
                "      ,'last_transaction_id'\n",
                "      ,'last_transaction_ip'\n",
                "      ,'total_orders'\n",
                "      ,'total_units'\n",
                "      ,'total_gross'\n",
                "      ,'total_discounts'\n",
                "      ,'total_returns'\n",
                "      ,'total_shipping'\n",
                "      ,'total_taxes'\n",
                "      ,'r_score'\n",
                "      ,'f_score'\n",
                "      ,'m_score'\n",
                "      ,'rfm_score'\n",
                "      ,'is_reseller'\n",
                "      ,'avg_sku_count'\n",
                "      ]"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ea4fd7c0-4493-4366-849a-e0148436294f",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "37e9ec58-0ac7-407e-8e2b-e117ba0bce5a",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create Custom Features"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "deae9041-4f64-4b39-9c37-de9fcba01b68",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "d6a32e88-f773-4ca8-af14-eacbb74b0cdf"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pyspark.sql.functions as F\n",
                "\n",
                "# create 4 custom features\n",
                "df = (\n",
                "    customers.select(\n",
                "        [customers[col] for col in input_columns]\n",
                "        + [\n",
                "            F.substring_index(\n",
                "                F.substring_index(F.lower(customers[\"email\"]), \"@\", -1), \".\", 1\n",
                "            ).alias(\"email_domain\")\n",
                "        ]\n",
                "        + [\n",
                "            F.concat(\n",
                "                F.coalesce(F.lower(customers[\"last_shipping_address_address1\"]), F.lit(\"\")),\n",
                "                F.lit(\" \"),\n",
                "                F.coalesce(F.lower(customers[\"last_shipping_address_address2\"]), F.lit(\"\")),\n",
                "                F.lit(\" \"),\n",
                "                F.coalesce(F.lower(customers[\"last_shipping_address_city\"]), F.lit(\"\")),\n",
                "                F.lit(\" \"),\n",
                "                F.coalesce(F.lower(customers[\"last_shipping_address_country_code\"]), F.lit(\"\")),\n",
                "                F.lit(\" \"),\n",
                "                F.coalesce(F.lower(customers[\"last_shipping_address_zip\"]), F.lit(\"\")),\n",
                "            ).alias(\"address\")\n",
                "        ]\n",
                "    )\n",
                "    .withColumn(\"email_domain_address\", F.array(\"email_domain\", \"address\"))\n",
                "    .withColumn(\"units_per_order\", F.col(\"total_units\") * 1.0 / F.col(\"total_orders\"))\n",
                "    .withColumn(\"gross_per_order\", F.col(\"total_gross\") * 1.0 / F.col(\"total_orders\"))\n",
                "    .withColumn(\"units_per_sku\", F.col('units_per_order') * 1.0 / F.col('avg_sku_count'))\n",
                ")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "e0668a98-b3a8-451b-ab6f-b607db182ee5",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "4c5b2f72-dbce-403d-9e66-1647dda02ba5",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "IDENTIFIERS = ['external_customer_id', 'email']\n",
                "CONTINUOUS_COLUMNS = [\n",
                "  'total_orders',\n",
                "  'total_units',\n",
                "  'total_gross',\n",
                "  'total_discounts',\n",
                "  'total_returns',\n",
                "  'total_shipping',\n",
                "  'total_taxes',\n",
                "  'r_score',\n",
                "  'f_score',\n",
                "  'rfm_score',\n",
                "  'avg_sku_count',\n",
                "  'units_per_order', \n",
                "  'gross_per_order', \n",
                "  'units_per_sku'\n",
                "]\n",
                "CATEGORICAL_COLUMN = 'email_domain_address'\n",
                "TARGET_COLUMN = ['is_reseller']"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b05c1d72-4dfb-4492-b918-c2c9126d54c3",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "c8119232-0f40-459c-8cc1-21c8c63f836e",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Drop nulls\n",
                "customers = df.dropna(\n",
                "  how='any',\n",
                "  subset=[x for x in IDENTIFIERS + CONTINUOUS_COLUMNS + TARGET_COLUMN + [CATEGORICAL_COLUMN]]\n",
                ")\n",
                "\n",
                "# Remove duplicates\n",
                "customers = customers.dropDuplicates(subset=['sales_channel_id', 'external_customer_id']) "
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "adeb6073-2936-4cc0-8778-ecd18aa89310",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "7e8fb0d2-2f42-438d-9234-526ef068e983",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Train Test Split"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "76fd7d86-f0fa-43ec-ac45-6db2c40ee8d7",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "1b05de06-b205-4113-85aa-3d97cf696218"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Take all the US resellers\n",
                "resellers = customers.where('sales_channel_id = 1 and is_reseller = 1 and CAST(last_transaction_date AS DATE) <= \"2023-05-28\"')\n",
                "# Take US normal customers in 2023 only\n",
                "normal_customers = customers.where('sales_channel_id = 1 and is_reseller = 0 and CAST(last_transaction_date AS DATE) between \"2023-01-01\" and \"2023-05-28\" ')\n",
                "\n",
                "# Split the resellers and normal customers, then merge them together, and then suffle the order. \n",
                "# This ensures train, test, val dataset have equal portion of resellers and normal customers.\n",
                "train_resellers, test_resellers, val_resellers = resellers.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
                "train_normal, test_normal, val_normal = normal_customers.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
                "\n",
                "train = train_resellers.union(train_normal).orderBy(F.rand(seed=42))\n",
                "test = test_resellers.union(test_normal).orderBy(F.rand(seed=42))\n",
                "val = val_resellers.union(val_normal).orderBy(F.rand(seed=42))\n",
                "\n",
                "train.cache()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "376c2cd5-8865-418e-a096-415b9ce6ca48",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "1ecc1077-7f58-4f98-86a5-99b89a6e67cd",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 64,
                    "data": {
                        "text/plain": "DataFrame[sales_channel_id: int, external_customer_id: bigint, customer_type: string, email: string, first_name: string, last_name: string, last_shipping_address_address1: string, last_shipping_address_address2: string, last_shipping_address_city: string, last_shipping_address_country: string, last_shipping_address_phone: string, last_shipping_address_province: string, last_shipping_address_zip: string, last_shipping_address_country_code: string, last_shipping_address_province_code: string, first_transaction_date: timestamp, first_transaction_id: bigint, last_transaction_date: timestamp, last_transaction_id: bigint, last_transaction_ip: string, total_orders: int, total_units: int, total_gross: decimal(19,4), total_discounts: decimal(19,4), total_returns: decimal(19,4), total_shipping: decimal(19,4), total_taxes: decimal(19,4), r_score: int, f_score: int, m_score: int, rfm_score: int, is_reseller: int, avg_sku_count: double, email_domain: string, address: string, email_domain_address: array<string>, units_per_order: double, gross_per_order: double, units_per_sku: double]"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create a Pipeline"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "a95dcbe7-2a87-4361-8fc7-fc438cb8a832",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "b2724847-8617-4c02-a401-e2161984b5b1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "imput_columns = [(x + '_i') for x in CONTINUOUS_COLUMNS if x not in ['units_per_order', 'gross_per_order', 'units_per_sku']]"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0edca4f9-fc28-40d1-b5ba-bf34a1d9fbd3",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "13f67e9c-93bc-4004-91f1-e3f8af83b138",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml import Pipeline\n",
                "import pyspark.ml.feature as MF\n",
                "\n",
                "embedding_size = 100\n",
                "\n",
                "imputer = MF.Imputer(\n",
                "  strategy='mean',\n",
                "  inputCols=[x for x in CONTINUOUS_COLUMNS if x not in ['units_per_order', 'gross_per_order', 'units_per_sku']],\n",
                "  outputCols=imput_columns\n",
                ")\n",
                "\n",
                "embedding = MF.Word2Vec(\n",
                "    vectorSize=embedding_size,\n",
                "    inputCol='email_domain_address',\n",
                "    outputCol='embedded'\n",
                ")\n",
                "\n",
                "continuous_assembler = MF.VectorAssembler(\n",
                "  inputCols=imput_columns + ['units_per_order', 'gross_per_order', 'units_per_sku', 'embedded'],\n",
                "  outputCol='continuous'\n",
                ")\n",
                "\n",
                "continuous_scaler = MF.StandardScaler(\n",
                "  inputCol='continuous',\n",
                "  outputCol='features'\n",
                ")\n",
                "\n",
                "customers_pipeline = Pipeline(\n",
                "  stages=[imputer, embedding, continuous_assembler, continuous_scaler]\n",
                ")\n",
                "\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "customers_features = customers_pipeline_model.transform(train)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "5d3cf9eb-d9da-438e-8353-baf6f4ef3a2b",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "bb4472b8-4ddf-4ad7-9435-79d795848382",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##LogisticRegression"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "88cc1de2-40b7-44cf-b39a-3240dbf6e53b",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "9f263fd3-aab7-44a3-a6db-b2145f8bceab"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.classification import LogisticRegression\n",
                "import mlflow\n",
                "\n",
                "clf = LogisticRegression(\n",
                "  featuresCol='features',\n",
                "  labelCol='is_reseller',\n",
                "  predictionCol='prediction'\n",
                ")\n",
                "\n",
                "customers_pipeline.setStages(\n",
                "  [\n",
                "    imputer, \n",
                "    embedding,\n",
                "    continuous_assembler, \n",
                "    continuous_scaler,\n",
                "    clf\n",
                "  ]\n",
                ")\n",
                "\n",
                "# Start an MLflow run\n",
                "mlflow.start_run()\n",
                "\n",
                "# Train a model\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "\n",
                "# Predictions\n",
                "results = customers_pipeline_model.transform(val)\n",
                "\n",
                "# Log metrics\n",
                "model = customers_pipeline_model.stages[-1]\n",
                "metrics = model.evaluate(results.select('email', 'is_reseller', 'features'))\n",
                "\n",
                "mlflow.log_metric(\"val_accuracy\", metrics.accuracy)\n",
                "mlflow.log_metric(\"val_precision\", metrics.precisionByLabel[1])\n",
                "mlflow.log_metric(\"val_recall\", metrics.recallByLabel[1])\n",
                "\n",
                "# Log the model\n",
                "mlflow.spark.log_model(customers_pipeline_model, \"LogisticRegression_model\")\n",
                "\n",
                "# Close the MLflow run\n",
                "mlflow.end_run()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "9fcec4be-c1d2-4b64-aa64-19346163ee43",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "277ca6db-34c0-47dd-9788-6ad03a3dc8a4",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/06/04 23:22:13 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##RandomForest"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "2c377111-46c9-4a8b-bcfe-023761642465",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "206bf9ee-cb1f-4af3-a48f-ed882d4faa09"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.classification import RandomForestClassifier\n",
                "\n",
                "clf = RandomForestClassifier(\n",
                "  featuresCol='features',\n",
                "  labelCol='is_reseller',\n",
                "  predictionCol='prediction'\n",
                ")\n",
                "\n",
                "customers_pipeline.setStages(\n",
                "  [\n",
                "    imputer, \n",
                "    embedding,\n",
                "    continuous_assembler, \n",
                "    continuous_scaler,\n",
                "    clf\n",
                "  ]\n",
                ")\n",
                "\n",
                "# Start an MLflow run\n",
                "mlflow.start_run()\n",
                "\n",
                "# Train a model\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "\n",
                "# Predictions\n",
                "results = customers_pipeline_model.transform(val)\n",
                "\n",
                "# Log metrics\n",
                "model = customers_pipeline_model.stages[-1]\n",
                "metrics = model.evaluate(results.select('email', 'is_reseller', 'features'))\n",
                "\n",
                "mlflow.log_metric(\"val_accuracy\", metrics.accuracy)\n",
                "mlflow.log_metric(\"val_precision\", metrics.precisionByLabel[1])\n",
                "mlflow.log_metric(\"val_recall\", metrics.recallByLabel[1])\n",
                "\n",
                "# Log the model\n",
                "mlflow.spark.log_model(customers_pipeline_model, \"RandomForest_model\")\n",
                "\n",
                "# Close the MLflow run\n",
                "mlflow.end_run()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "47a13ce5-7ce5-4633-877e-2bf67f759753",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "3605afe6-fb30-4d86-b17f-95c78aee2c19",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/06/04 23:40:06 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##GBTClassifier"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d64d9757-85cf-4e6f-ab1b-16f5b4fc55f1",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "32e65d16-2dd7-4e5e-809f-d65aec57a1da"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import mlflow\n",
                "\n",
                "from pyspark.ml.classification import GBTClassifier\n",
                "from pyspark.mllib.evaluation import MulticlassMetrics\n",
                "\n",
                "clf = GBTClassifier(\n",
                "  featuresCol='features',\n",
                "  labelCol='is_reseller',\n",
                "  predictionCol='prediction'\n",
                ")\n",
                "\n",
                "customers_pipeline.setStages(\n",
                "  [\n",
                "    imputer, \n",
                "    embedding,\n",
                "    continuous_assembler, \n",
                "    continuous_scaler,\n",
                "    clf\n",
                "  ]\n",
                ")\n",
                "\n",
                "# Start an MLflow run\n",
                "mlflow.start_run()\n",
                "\n",
                "# Train a model\n",
                "customers_pipeline_model = customers_pipeline.fit(train)\n",
                "\n",
                "# Predictions\n",
                "results = customers_pipeline_model.transform(val)\n",
                "\n",
                "# Extract the predicted labels and true labels from the predictions DataFrame\n",
                "predictionAndLabels = results.select('prediction', F.col('is_reseller').cast(\"double\")).rdd\n",
                "\n",
                "# Create MulticlassMetrics object\n",
                "metrics = MulticlassMetrics(predictionAndLabels)\n",
                "\n",
                "# Log metrics\n",
                "mlflow.log_metric(\"val_accuracy\", metrics.accuracy)\n",
                "mlflow.log_metric(\"val_precision\", metrics.precision(1))\n",
                "mlflow.log_metric(\"val_recall\", metrics.recall(1))\n",
                "\n",
                "# Log the model\n",
                "mlflow.spark.log_model(customers_pipeline_model, \"GBT_model\")\n",
                "\n",
                "# Close the MLflow run\n",
                "mlflow.end_run()"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "dba82023-35b8-4ceb-a731-0659c90841b0",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "508cc836-7cb2-4aa4-87df-d807c64c69e0",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n2023/06/04 23:51:50 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "##Load Best Model"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "fa403485-f6cc-487f-ae83-d68628b65999",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "18318571-ee46-40b1-8522-45b51e95b3aa"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Validation dataset Evaluation"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "0ac16159-80f5-453e-8673-1b8d6e24afaa",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "8112c11c-0fcf-4111-b80e-0de9fb754449"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import mlflow\n",
                "logged_model = 'runs:/19f09a0394144282bdb142a6b4dc2627/LogisticRegression_model' # 'runs:/930ed5ed13e346c8a5fe59bfdd63e140/GBT_model' #'runs:/1ae9a83859684823aede473a5e5a21d2/LogisticRegression_model' #'runs:/a2b45aae0d854e5b81cb34caebd12e07/GBT_model'\n",
                "\n",
                "# Load model\n",
                "loaded_model = mlflow.spark.load_model(logged_model)\n",
                "\n",
                "# Perform inference via model.transform()\n",
                "results = loaded_model.transform(val)\n",
                "\n",
                "# Extract the predicted labels and true labels from the predictions DataFrame\n",
                "predictionAndLabels = results.select('prediction', F.col('is_reseller').cast(\"double\")).rdd\n",
                "\n",
                "# Create MulticlassMetrics object\n",
                "metrics = MulticlassMetrics(predictionAndLabels)\n",
                "\n",
                "# validation metrics\n",
                "print(f'Accuracy: {metrics.accuracy:.5f}', f'Precision: {metrics.precision(1):.5f}', f'Recall: {metrics.recall(1):.5f}', f\"F1: {metrics.fMeasure(1.0):.5f}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d59105a3-9570-406f-b5b6-6312abacd1ed",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "4f1cfecd-8950-46a3-9b3d-d9ce49f3ac60",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/06/05 00:02:09 INFO mlflow.spark: 'runs:/19f09a0394144282bdb142a6b4dc2627/LogisticRegression_model' resolved as 'dbfs:/databricks/mlflow-tracking/4401469133110519/19f09a0394144282bdb142a6b4dc2627/artifacts/LogisticRegression_model'\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Accuracy: 0.99905 Precision: 0.96000 Recall: 0.82759 F1: 0.88889\n"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Test dataset Evaluation"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ea6709ec-cbac-4380-bb31-faebbcf55bad",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "ac74b853-1e96-4dee-97dd-837ef22ceaaa"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import mlflow\n",
                "logged_model = 'runs:/19f09a0394144282bdb142a6b4dc2627/LogisticRegression_model'\n",
                "\n",
                "# Load model\n",
                "loaded_model = mlflow.spark.load_model(logged_model)\n",
                "\n",
                "# Perform inference via model.transform()\n",
                "results = loaded_model.transform(test)\n",
                "\n",
                "# Extract the predicted labels and true labels from the predictions DataFrame\n",
                "predictionAndLabels = results.select('prediction', F.col('is_reseller').cast(\"double\")).rdd\n",
                "\n",
                "# Create MulticlassMetrics object\n",
                "metrics = MulticlassMetrics(predictionAndLabels)\n",
                "\n",
                "# Validation metrics\n",
                "print(f'Accuracy: {metrics.accuracy:.5f}', f'Precision: {metrics.precision(1):.5f}', f'Recall: {metrics.recall(1):.5f}', f\"F1: {metrics.fMeasure(1.0):.5f}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "69cc162f-a9e6-4daa-9a84-b1833d36fa8b",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "a3d7e1e0-1b9e-46e6-89b3-1b4576aa93a7",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "2023/06/05 00:04:21 INFO mlflow.spark: 'runs:/19f09a0394144282bdb142a6b4dc2627/LogisticRegression_model' resolved as 'dbfs:/databricks/mlflow-tracking/4401469133110519/19f09a0394144282bdb142a6b4dc2627/artifacts/LogisticRegression_model'\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Accuracy: 0.99865 Precision: 0.92727 Recall: 0.85000 F1: 0.88696\n"
                }
            ],
            "execution_count": null
        }
    ]
}